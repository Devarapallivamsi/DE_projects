{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lit, current_timestamp, sum as _sum\n",
        "from delta.tables import DeltaTable\n",
        "from pydeequ.checks import Check, CheckLevel\n",
        "from pydeequ.verification import VerificationSuite, VerificationResult\n",
        "import os"
      ],
      "metadata": {
        "id": "vA5e6ixHpKQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get job parameters from Databricks\n",
        "date_str = dbutils.widgets.get(\"current_date\")\n",
        "\n",
        "\n",
        "# Define file paths based on date parameter\n",
        "booking_data = f\"dbfs:/DataEngineering/bookings_daily_data/bookings_{date_str}.csv\"\n",
        "customer_data = f\"dbfs:/DataEngineering/customers_daily_data/customers_{date_str}.csv\""
      ],
      "metadata": {
        "id": "ZvjXebxapM9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read booking data\n",
        "booking_df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"quote\", \"\\\"\") \\\n",
        "    .option(\"multiLine\", \"true\") \\\n",
        "    .load(booking_data)\n",
        "\n",
        "booking_df.printSchema()\n",
        "# display(booking_data)\n",
        "\n",
        "\n",
        "customer_df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"quote\", \"\\\"\") \\\n",
        "    .option(\"multiLine\", \"true\") \\\n",
        "    .load(customer_data)\n",
        "\n",
        "customer_df.printSchema()\n",
        "display(customer_df)"
      ],
      "metadata": {
        "id": "yD4NO-NlpXKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Quality Checks on booking data\n",
        "check_incremental = Check(spark, CheckLevel.Error, \"Booking Data Check\") \\\n",
        "    .hasSize(lambda x: x > 0) \\\n",
        "    .isUnique(\"booking_id\", hint=\"Booking ID is not unique throught\") \\\n",
        "    .isComplete(\"customer_id\") \\\n",
        "    .isComplete(\"amount\") \\\n",
        "    .isNonNegative(\"amount\") \\\n",
        "    .isNonNegative(\"quantity\") \\\n",
        "    .isNonNegative(\"discount\")\n",
        "\n",
        "# Data Quality Checks on customer data\n",
        "check_scd = Check(spark, CheckLevel.Error, \"Customer Data Check\") \\\n",
        "    .hasSize(lambda x: x > 0) \\\n",
        "    .isUnique(\"customer_id\") \\\n",
        "    .isComplete(\"customer_name\") \\\n",
        "    .isComplete(\"customer_address\") \\\n",
        "    .isComplete(\"phone_number\") \\\n",
        "    .isComplete(\"email\")\n",
        "\n",
        "# Run the verification suite\n",
        "booking_dq_check = VerificationSuite(spark) \\\n",
        "    .onData(booking_df) \\\n",
        "    .addCheck(check_incremental) \\\n",
        "    .run()\n",
        "\n",
        "customer_dq_check = VerificationSuite(spark) \\\n",
        "    .onData(customer_df) \\\n",
        "    .addCheck(check_scd) \\\n",
        "    .run()"
      ],
      "metadata": {
        "id": "7KVYiZYspf7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "booking_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, booking_dq_check)\n",
        "display(booking_dq_check_df)\n",
        "\n",
        "customer_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, customer_dq_check)\n",
        "display(customer_dq_check_df)\n",
        "\n",
        "# Check if verification passed\n",
        "if booking_dq_check.status != \"Success\":\n",
        "    raise ValueError(\"Data Quality Checks Failed for Booking Data\")\n",
        "\n",
        "if customer_dq_check.status != \"Success\":\n",
        "    raise ValueError(\"Data Quality Checks Failed for Customer Data\")"
      ],
      "metadata": {
        "id": "ZsEXeJAqpoeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add ingestion timestamp to booking data\n",
        "booking_df_incremental = booking_df.withColumn(\"ingestion_time\", current_timestamp())\n",
        "\n",
        "# Join booking data with customer data\n",
        "df_joined = booking_df_incremental.join(customer_df, \"customer_id\")\n",
        "\n",
        "# Business transformation: calculate total cost after discount and filter\n",
        "df_transformed = df_joined \\\n",
        "    .withColumn(\"total_cost\", col(\"amount\") - col(\"discount\")) \\\n",
        "    .filter(col(\"quantity\") > 0)\n",
        "\n",
        "# Group by and aggregate df_transformed\n",
        "df_transformed_agg = df_transformed \\\n",
        "    .groupBy(\"booking_type\", \"customer_id\") \\\n",
        "    .agg(\n",
        "        _sum(\"total_cost\").alias(\"total_amount_sum\"),\n",
        "        _sum(\"quantity\").alias(\"total_quantity_sum\")\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ztu7UtExrXG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the Delta table exists\n",
        "fact_table_path = \"gds_de_bootcamp.default.booking_fact\"\n",
        "fact_table_exists = spark._jsparkSession.catalog().tableExists(fact_table_path)\n",
        "\n",
        "if fact_table_exists:\n",
        "    # Read the existing fact table\n",
        "    df_existing_fact = spark.read.format(\"delta\").table(fact_table_path)\n",
        "\n",
        "    # Combine the aggregated data\n",
        "    df_combined = df_existing_fact.unionByName(df_transformed_agg, allowMissingColumns=True)\n",
        "\n",
        "    # Perform another group by and aggregation on the combined data\n",
        "    df_final_agg = df_combined \\\n",
        "        .groupBy(\"booking_type\", \"customer_id\") \\\n",
        "        .agg(\n",
        "            _sum(\"total_amount_sum\").alias(\"total_amount_sum\"),\n",
        "            _sum(\"total_quantity_sum\").alias(\"total_quantity_sum\")\n",
        "        )\n",
        "else:\n",
        "    # If the fact table doesn't exist, use the aggregated transformed data directly\n",
        "    df_final_agg = df_transformed_agg\n",
        "\n",
        "# display(df_final_agg)\n",
        "\n",
        "# Write the final aggregated data back to the Delta table\n",
        "df_final_agg.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(fact_table_path)"
      ],
      "metadata": {
        "id": "rBPCALRYrboh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0edea227-94b8-4f08-b350-b5202dc1c145",
          "showTitle": false,
          "title": ""
        },
        "id": "HU6mrMuOpF99"
      },
      "outputs": [],
      "source": [
        "scd_table_path = \"gds_de_bootcamp.default.customer_scd\"\n",
        "scd_table_exists = spark._jsparkSession.catalog().tableExists(scd_table_path)\n",
        "\n",
        "# Check if the customers table exists\n",
        "if scd_table_exists:\n",
        "    # Load the existing SCD table\n",
        "    scd_table = DeltaTable.forName(spark, scd_table_path)\n",
        "    display(scd_table.toDF())\n",
        "\n",
        "    # Perform SCD2 merge logic\n",
        "    scd_table.alias(\"scd\") \\\n",
        "        .merge(\n",
        "            source=customer_df.alias(\"updates\"),\n",
        "            condition=\"scd.customer_id = updates.customer_id and scd.valid_to = '9999-12-31'\"\n",
        "        ) \\\n",
        "        .whenMatchedUpdate(set={\n",
        "            \"valid_to\": \"updates.valid_from\",\n",
        "        }) \\\n",
        "        .execute()\n",
        "\n",
        "    customer_df.write.format(\"delta\").mode(\"append\").saveAsTable(scd_table_path)\n",
        "else:\n",
        "    # If the SCD table doesn't exist, write the customer data as a new Delta table\n",
        "    customer_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(scd_table_path)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 901566473457969,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "icremental_booking_data_processing",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}